{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# sys.path.insert(1, '../dataset')\n",
    "# import data_preparation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from utils import tag_to_word_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Sentiment analysis, i.e. the analysis of the feeling expressed in a sentence, is a leading application area in natural language processing (NLP). Indeed, it has attracted the interest of brands, which are interested in analyzing customer feedback, such as opinions in survey responses and social media conversations, so that they can tailor products and services to meet their customersâ€™ needs.\n",
    "\n",
    "Nevertheless, basic sentiment analysis attempts to detect the overall polarity of a sentence irrespective of the entities mentioned and their aspects. Therefore, a new task has been introduced: Aspect-based sentiment analysis (ABSA), a text analysis technique that categorizes data by aspect and identifies the sentiment attributed to each one. \n",
    "\n",
    "In this notebook, we provide a possible solution to two steps of ABSA:\n",
    "1. Aspect term extraction (ATE);\n",
    "2. Aspect-based sentiment analysis (ABSA).\n",
    "\n",
    "# Task description\n",
    "\n",
    "Let us consider a given sentence $s$, which for instance could come from a review of a product or a social media post.\n",
    "A company could be interested in getting to know the major sentiment of the sentence, i.e. whether it is positive, negative or neutral, w.r.t. each of the most important aspects of the sentence.\n",
    "\n",
    "For example, the sentence \"The food was delicious, but the prices were a bit high\" has two different polarities to point out, each of which is attributed to a different aspect. Indeed, the sentiment concerning the **food** is positive, while the sentiment concerning the **prices** is negative, and both these informations could be separately interesting to a company.\n",
    "\n",
    "Thus, in order to extract such information, we need to first identify the aspects of the sentence, and then extract the sentiment associated to each of them. These are the two separated tasks we are going to approach in this notebook.\n",
    "\n",
    "1. Aspect-based term extraction (ABTE): given sentence, identify all apect terms present in the sentence;\n",
    "2. Aspect-based sentiment analysis (ABSA): given sentence and an aspect term, identify the sentiment associated to that aspect term.\n",
    "\n",
    "Clearly, the first task can be seen as a preprocessing step in order to perform the second task, thus, if we deal with data in which the aspects are not manually annotated we should perform ATE and then ABSA. On the other hand, if the aspects are already manually annotated, we can directly perform ABSA in order to extract the sentiment associated to each aspect.\n",
    "\n",
    "\n",
    "# Dataset\n",
    "\n",
    "In order to train and test our models we use a dataset containing restaurant reviews, taken from a preprocessed version, available at [1], of the SemEval-2014 ABSA Task [2].\n",
    "\n",
    "Data are organized in a csv file, with the following columns:\n",
    "- **Tokens**: tokenized sentence;\n",
    "- **Tags**: list of tags associated to each token: '0' for non-aspect terms, '1' for beginning of terms and '2' for marks of terms;\n",
    "- **Polarities**: list of polarities associated to each token: '0' for negative, '1' for neutral and '2' for positive and '-1' for non-aspect terms;\n",
    "\n",
    "# Models\n",
    "\n",
    "To solve the problems described above, we provide different strategies, all based on the fine-tuning of a pretrained BERT model [3], with the implementation provided by Hugging Face [4].\n",
    "BERT is a bidirectional transformer pretrained using a combination of masked language modeling objective and next sentence prediction, which can be adapted to a wide range of tasks, including sentiment analysis.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "Both tasks are approached with two different BERT based approaches: a straighforward fine-tuning and an adapter, implementation provided by AdapterHub[5].\n",
    "\n",
    "1. **Fine-tuning** consist of taking pretrained model ('bert-base-uncased' in our case) and train in specifically for ATE and ABSE. Thus, the idea is to update the entire copy of the original pretrained model, which turns out to be not efficient. See Figure 1 below [3] for a summary of the model architecture for fine-tuning:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/fine_tuning.png\" width=\"800\" />\n",
    "</p>\n",
    "\n",
    "2. **Adapter modules** have been introduced [6] as a more efficient approach than fine-tuning. In this scenario, the parameters of the original model are fixed, and one has to train only a few trainable parameters per task: these new task-specific parameters are called adaptors. See Figure 1 below [6] for a summary of the adapter architecture: \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/adapter_architecture.png\" width=\"850\" />\n",
    "</p>\n",
    "\n",
    "In particular, for ATE we feed the transformer with the list of indices of words in the vocabulary. On the other hand, in ABSE we first concatenate the aspect with the list of sentence tokens as follows:\n",
    "```{note}\n",
    "    ['w1', 'w2', 'w3', ... , 'wN', '[CLS]', 'aspect']\n",
    "```\n",
    "Then we feed the transformer with the list of indices of words in the vocabulary.\n",
    "\n",
    "\n",
    "### Optimization strategy and training\n",
    "\n",
    "Finally, we provide two different optimization strategies, both based on the AdamW algorithm [7], with the implementation provided in Pytorch [8].\n",
    "Moreover, we test both AdamW with a fixed learning rate and with a a learning rate scheduler, linear for ATE and polynomial for ABSE.\n",
    "\n",
    "Training is performed with a batch size of 8, and 5 epochs for all the cases, as suggested for BERT models. While ATE is trained with a $3\\,10^{-5}$ learning rate, ABSE is trained with a $10^{-3}$ learning rate since the second approach shows a less stable trend.\n",
    "\n",
    "## Overview\n",
    "The notebook is structured as follows:\n",
    "1. Text preprocessing and normalization;\n",
    "2. Aspect term extraction: training overview and comparison, testing, evaluation, and visualization;\n",
    "3. Aspect-based sentiment analysis:  training overview and comparison, testing, evaluation, and visualization;\n",
    "4. Conclusions and comparison with other results.\n",
    "\n",
    "Since we trained 8 different models, training has been performed separately and loaded in the present notebook. Nevertheless, we show the process of training by means of a plot of the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data\n",
    "# if not os.path.exists('../dataset/prepared'):\n",
    "#     os.makedirs('../dataset/prepared')\n",
    "# data_preparation.to_csv('../dataset/atepc/restaurants_test.csv', '../dataset/prepared/restaurants_test.csv',)\n",
    "# data_preparation.to_csv('../dataset/atepc/restaurants_train.csv', '../dataset/prepared/restaurants_train.csv',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Text normalization\n",
    "\n",
    "We first perform a basic text normalization, fixing text contractions and performing spell checking by means of textblob and contractions. Results are then saved in a dedicated folder with the same format of original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spelling check\n",
    "from textblob import TextBlob\n",
    "import contractions\n",
    "\n",
    "def normalize(data):\n",
    "    d = data\n",
    "    def correct(token_list):\n",
    "        l = token_list.replace(\"'\", \"\").strip(\"][\").split(', ')\n",
    "        # abbreviations\n",
    "        l = [contractions.fix(ll) for ll in l ]\n",
    "        s = ' '.join(l)\n",
    "        s = str(TextBlob(s).correct().words)\n",
    "        return s \n",
    "    d['Tokens'] = d['Tokens'].apply(lambda x: correct(x))\n",
    "    return d\n",
    "\n",
    "# #load\n",
    "# data = pd.read_csv('dataset/restaurants_train.csv')\n",
    "# data_test = pd.read_csv('dataset/restaurants_test.csv')\n",
    "\n",
    "# data_test = normalize(data_test)\n",
    "# data = normalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #normalized data\n",
    "# if not os.path.exists('../dataset/normalized'):\n",
    "#     os.makedirs('../dataset/normalized')\n",
    "# data_test.to_csv('../dataset/normalized/restaurants_test.csv', index=False)\n",
    "# data.to_csv('../dataset/normalized/restaurants_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Aspect term extraction\n",
    "Let us consider the 4 scenarios already discussed (fine-tuning, adapter, fine-tuning with scheduler, adapter with scheduler) and load the models trained with 5 epochs, batch size 8, and a learning rate of $3\\,10^{-5}$.\n",
    "\n",
    "For each case, we compute test accuracy, test classification report, predict the labels and save all the results. Same process is done for training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 14:18:23.343073: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-08 14:18:23.361036: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749363503.381894 3285319 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749363503.388205 3285319 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749363503.403999 3285319 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749363503.404018 3285319 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749363503.404020 3285319 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749363503.404022 3285319 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-08 14:18:23.409422: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from abte import ABTEModel\n",
    "\n",
    "batch = 8\n",
    "lr = 3*1e-5\n",
    "epochs = 5\n",
    "\n",
    "def run_ABTE_test_train(adapter, lr_schedule):\n",
    "    if adapter:\n",
    "        if lr_schedule: dir_name  = \"model_ABTE_adapter_scheduler\"\n",
    "        else: dir_name = \"model_ABTE_adapter\"\n",
    "    else:\n",
    "        if lr_schedule: dir_name  = \"model_ABTE_scheduler\"\n",
    "        else: dir_name = \"model_ABTE\"\n",
    "\n",
    "    #load\n",
    "    data = pd.read_csv('../dataset/normalized/restaurants_train.csv')\n",
    "    data_test = pd.read_csv('../dataset/normalized/restaurants_test.csv')\n",
    "\n",
    "    # define parameters for model\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"/home/STU/ljq/Projects/BERT-ABSA/bert-base-uncased\")\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # define model\n",
    "    modelABTE = ABTEModel(tokenizer, adapter=adapter)\n",
    "\n",
    "    # load model and predict\n",
    "    model_path = dir_name+'/model_lr3.0000000000000004e-05_epochs4_batch8.pkl'\n",
    "    test_accuracy, test_report = modelABTE.test(data_test, load_model=model_path, device=DEVICE)\n",
    "    test_pred, test_targets = modelABTE.predict_batch(data_test, load_model=model_path, device=DEVICE)\n",
    "\n",
    "    train_accuracy, train_report = modelABTE.test(data, load_model=model_path, device=DEVICE)\n",
    "    train_pred, train_targets = modelABTE.predict_batch(data, load_model=model_path, device=DEVICE)\n",
    "\n",
    "    #save results\n",
    "    if not os.path.exists('/results'):\n",
    "        os.makedirs(dir_name+'/results')\n",
    "\n",
    "    #report\n",
    "    with open(dir_name+'/results/test_report_lr{}_epochs{}_batch{}.csv'.format(lr, epochs, batch), 'w') as f:\n",
    "        for r in test_report.split('\\n'):\n",
    "            f.write(r + '\\n')\n",
    "\n",
    "    with open(dir_name+'/results/train_report_lr{}_epochs{}_batch{}.csv'.format(lr, epochs, batch), 'w') as f:\n",
    "        for r in train_report.split('\\n'):\n",
    "            f.write(r + '\\n')\n",
    "\n",
    "    #predictions\n",
    "    data_test['Predicted'] = test_pred\n",
    "    data_test['Actual'] = test_targets\n",
    "    data_test.to_csv(dir_name+'/results/test_pred_lr{}_epochs{}_batch{}.csv'.format(lr, epochs, batch), index=False)\n",
    "\n",
    "    data['Predicted'] = train_pred\n",
    "    data['Actual'] = train_targets\n",
    "    data.to_csv(dir_name+'/results/train_pred_lr{}_epochs{}_batch{}.csv'.format(lr, epochs, batch), index=False)\n",
    "\n",
    "    #accuracy\n",
    "    test_accuracy = np.array(test_accuracy)\n",
    "    train_accuracy = np.array(train_accuracy)\n",
    "\n",
    "    with open(dir_name+'/results/test_accuracy_lr{}_epochs{}_batch{}.csv'.format(lr, epochs, batch), 'w') as f:\n",
    "        f.write(str(test_accuracy))\n",
    "    with open(dir_name+'/results/train_accuracy_lr{}_epochs{}_batch{}.csv'.format(lr, epochs, batch), 'w') as f:\n",
    "        f.write(str(train_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/STU/ljq/Projects/BERT-ABSA/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Model not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_ABTE_test_train\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m run_ABTE_test_train(\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m run_ABTE_test_train(\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m, in \u001b[0;36mrun_ABTE_test_train\u001b[0;34m(adapter, lr_schedule)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# load model and predict\u001b[39;00m\n\u001b[1;32m     27\u001b[0m model_path \u001b[38;5;241m=\u001b[39m dir_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/model_lr3.0000000000000004e-05_epochs4_batch8.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 28\u001b[0m test_accuracy, test_report \u001b[38;5;241m=\u001b[39m \u001b[43mmodelABTE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m test_pred, test_targets \u001b[38;5;241m=\u001b[39m modelABTE\u001b[38;5;241m.\u001b[39mpredict_batch(data_test, load_model\u001b[38;5;241m=\u001b[39mmodel_path, device\u001b[38;5;241m=\u001b[39mDEVICE)\n\u001b[1;32m     31\u001b[0m train_accuracy, train_report \u001b[38;5;241m=\u001b[39m modelABTE\u001b[38;5;241m.\u001b[39mtest(data, load_model\u001b[38;5;241m=\u001b[39mmodel_path, device\u001b[38;5;241m=\u001b[39mDEVICE)\n",
      "File \u001b[0;32m~/Projects/BERT-ABSA/src/abte.py:227\u001b[0m, in \u001b[0;36mABTEModel.test\u001b[0;34m(self, dataset, load_model, device)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, load_model)\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 227\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel not found\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrained:\n",
      "\u001b[0;31mException\u001b[0m: Model not found"
     ]
    }
   ],
   "source": [
    "run_ABTE_test_train(False, False)\n",
    "run_ABTE_test_train(False, True)\n",
    "run_ABTE_test_train(True, False)\n",
    "run_ABTE_test_train(True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a Training history\n",
    "\n",
    "In the following figure, we plot the training loss and the validation loss for the fine-tuning and adapter cases, both with and without scheduler. Let us point out that both panels represent the same data, but in the right panel the loss function trend is shown in log scale. \n",
    "\n",
    "We can observe that all of the models exhibit a similar behavior, with several oscillations in the loss function trend. Nevertheless, we could point out (see left panel) that the combination of adapter and scheduler allows us to reduce the amplitude of the oscillations, leading to more stable results and a lower loss function in general.\n",
    "\n",
    "Note also that the first couple of iterations show a fast decay, while as training progresses, the decreasing trend gets slower. Finally, the log scaled plot shows that the learning curve starts to get flat during the last epoch, thus we fix 5 epochs in order to avoid overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossABTE = np.loadtxt('model_ABTE/losses_lr3.0000000000000004e-05_epochs5_batch8.txt')\n",
    "lossABTE_AS = np.loadtxt('model_ABTE_adapter_scheduler/losses_lr3.0000000000000004e-05_epochs5_batch8.txt')\n",
    "lossABTE_S = np.loadtxt('model_ABTE_scheduler/losses_lr3.0000000000000004e-05_epochs5_batch8.txt')\n",
    "lossABTE_A = np.loadtxt('model_ABTE_adapter/losses_lr3.0000000000000004e-05_epochs5_batch8.txt')\n",
    "\n",
    "sns.set_theme (style=\"white\", rc={\"lines.linewidth\": 3}, font_scale=1.5, palette=\"Dark2\")\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "\n",
    "for i in [0,1]:\n",
    "    sns.lineplot(range(len(lossABTE)), lossABTE, ax=ax[i], label = 'Fine tuning')\n",
    "    sns.lineplot(range(len(lossABTE_S)), lossABTE_S, ax=ax[i], label = 'Fine tuning +\\nscheduler')\n",
    "    sns.lineplot(range(len(lossABTE_A)), lossABTE_A, ax=ax[i], label = 'Adapter')\n",
    "    sns.lineplot(range(len(lossABTE_AS)), lossABTE_AS, ax=ax[i], label = 'Adapter +\\nscheduler')\n",
    "\n",
    "    ax[i].set_xlabel('iteration')\n",
    "    ax[i].set_ylabel('loss')\n",
    "ax[1].set_yscale('log')\n",
    "ax[1].set_ylabel('log loss')\n",
    "ax[0].legend().set_visible(False)\n",
    "ax[1].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "if not os.path.isdir('results_ABTE'):\n",
    "    os.makedirs('results_ABTE')\n",
    "fig.savefig('results_ABTE/loss_lr{:.5f}_epochs{}_batch{}.pdf'.format(lr, epochs, batch), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b Classification reports\n",
    "\n",
    "Here we propose the classification reports for all the four models trained in the previous sections.\n",
    "In particular we can make the following observations:\n",
    "\n",
    "1. **FINE-TUNING**: the model tends to overlearn, since all the metrics concerning the train dataset are close to 1, while performances on the test dataset are low in terms of f1-score. The 0.92 accuracy of test data is mainly due to correct identification of non-aspect terms, which is not the matter of interest;\n",
    "\n",
    "2. **FINE-TUNING WITH SCHEDULER**: introducing the scheduling prevents the model from overlearning, and all metrics show comparable performances on train and test dataset. In general, the task is accomplished with good results in terms of accuracy, f1-score is greater than or equal to 0.65.\n",
    "\n",
    "3. **ADAPTER**: this approach shows similar results to fine-tuning+scheduler, oscillations may tend to increase precision and reduce recall or vice-versa, but still with similar f1-score.\n",
    "\n",
    "4. **ADAPTER WITH SCHEDULER**: this approach shows the best results overall, with lowest overlearning tendency and higher precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import classification_report_read, print_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ATE = classification_report_read('model_ABTE/results/test_report_lr3.0000000000000004e-05_epochs5_batch8.csv')\n",
    "train_ATE = classification_report_read('model_ABTE/results/train_report_lr3.0000000000000004e-05_epochs5_batch8.csv')\n",
    "print_aligned(test_ATE, train_ATE, 'TEST FINE-TUNING', 'TRAIN FINE-TUNING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ATE_S = classification_report_read('model_ABTE_scheduler/results/test_report_lr3.0000000000000004e-05_epochs5_batch8.csv')\n",
    "train_ATE_S = classification_report_read('model_ABTE_scheduler/results/train_report_lr3.0000000000000004e-05_epochs5_batch8.csv')\n",
    "print_aligned(test_ATE_S, train_ATE_S, 'TEST FINE-TUNING + SCHEDULER', 'TRAIN FINE-TUNING + SCHEDULER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ATE_A = classification_report_read('model_ABTE_adapter/results/test_report_lr3.0000000000000004e-05_epochs5_batch8.csv')\n",
    "train_ATE_A = classification_report_read('model_ABTE_adapter/results/train_report_lr3.0000000000000004e-05_epochs5_batch8.csv')\n",
    "print_aligned(test_ATE_A, train_ATE_A, 'TEST ADAPTER', 'TRAIN ADAPTER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ATE_AS = classification_report_read('model_ABTE_adapter_scheduler/results/test_report_lr3.0000000000000004e-05_epochs5_batch8.csv')\n",
    "train_ATE_AS = classification_report_read('model_ABTE_adapter_scheduler/results/train_report_lr3.0000000000000004e-05_epochs5_batch8.csv')\n",
    "print_aligned(test_ATE_AS, train_ATE_AS, 'TEST ADAPTER + SCHEDULER', 'TRAIN ADAPTER + SCHEDULER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.c Results visualization\n",
    "In this section we propose a possible visualization of final results. First we show the final dataframe, with the comparison between the different sequences and the decoded tokens which correspond to gold terms and predicted terms.\n",
    "\n",
    "Finally, we show, by means of a word clod that the predicted terms are similar to the gold terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapter training \n",
    "dftest = pd.read_csv('model_ABTE_adapter/results/test_pred_lr3.0000000000000004e-05_epochs5_batch8.csv')\n",
    "test_pred = dftest['Predicted']\n",
    "\n",
    "#load\n",
    "data = pd.read_csv('../dataset/normalized/restaurants_train.csv')\n",
    "data_test = pd.read_csv('../dataset/normalized/restaurants_test.csv')\n",
    "\n",
    "test_tags_real = [t.strip('][').split(', ') for t in data_test['Tags']]\n",
    "test_tags_real = [[int(i) for i in t] for t in test_tags_real]\n",
    "\n",
    "test_pred = [t.strip('][').split(', ') for t in test_pred]\n",
    "test_pred = [[int(i) for i in t] for t in test_pred]\n",
    "\n",
    "\n",
    "ABTE_data = tag_to_word_df(data_test, 'gold terms', test_tags_real)\n",
    "ABTE_data = tag_to_word_df(ABTE_data, 'pred terms', test_pred)\n",
    "ABTE_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word clouds\n",
    "\n",
    "Visualization of the most frequent aspect words as a word cloud, target vs predicted. It is easy to notice that the two clouds are very similar, i.e. we can predict the correct aspect terms with a reasonable frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud (data):\n",
    "    from wordcloud import WordCloud\n",
    "    wordcloud = WordCloud( collocations=False,\n",
    "                          background_color=\"cornflowerblue\",\n",
    "                          colormap=\"magma\",\n",
    "                          max_words=50).generate(data)\n",
    "\n",
    "    return wordcloud\n",
    "\n",
    "def target_predicted_wordcloud(targets, predicted, file_name):\n",
    "    \n",
    "    sns.set_theme(style='white', font_scale=2)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(22, 6))\n",
    "    ax[0].imshow(word_cloud(targets))\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[0].set_title(\"Target\")\n",
    "    ax[1].imshow(word_cloud(predicted))\n",
    "    ax[1].axis(\"off\")\n",
    "    ax[1].set_title(\"Predicted\")\n",
    "    fig.savefig(file_name, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import target_predicted_wordcloud\n",
    "import itertools\n",
    "\n",
    "gold_terms = ABTE_data['gold terms'].values.flatten().tolist()\n",
    "gold_terms = list(itertools.chain(*gold_terms))\n",
    "\n",
    "pred_terms = ABTE_data['pred terms'].values.flatten().tolist()\n",
    "pred_terms = list(itertools.chain(*pred_terms))\n",
    "target_predicted_wordcloud(' '.join(gold_terms), ' '.join(pred_terms), \"results_ABTE/adapter_extracted_terms_wordcloud.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Aspect-based sentiment analysis\n",
    "we consider the 4 scenarios from above (fine-tuning, adapter, fine-tuning with scheduler, adapter with scheduler) and load the models trained with 5 epochs, batch size 8, and a learning rate  $10^{-5}$.\n",
    "\n",
    "For each case, we compute test accuracy, test classification report, predict the labels and save all the results. Same process is done for training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absa import ABSAModel\n",
    "\n",
    "#save results\n",
    "batch = 8\n",
    "lr = 1e-5\n",
    "epochs = 5\n",
    "\n",
    "def run_ABSA_test_train(adapter, lr_schedule):\n",
    "    if adapter:\n",
    "        if lr_schedule: dir_name_s  = \"model_ABSA_adapter_scheduler\"\n",
    "        else: dir_name_s = \"model_ABSA_adapter\"\n",
    "    else:\n",
    "        if lr_schedule: dir_name_s  = \"model_ABSA_scheduler\"\n",
    "        else: dir_name_s = \"model_ABSA\"\n",
    "\n",
    "    #load\n",
    "    data = pd.read_csv('../dataset/normalized/restaurants_train.csv')\n",
    "    data_test = pd.read_csv('../dataset/normalized/restaurants_test.csv')\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"/home/STU/ljq/Projects/BERT-ABSA/bert-base-uncased\")\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    modelABSA = ABSAModel(tokenizer, adapter=adapter)\n",
    "\n",
    "    model_path = dir_name_s+'/model_lr1e-05_epochs4_batch8.pkl'\n",
    "    test_accuracy, test_report = modelABSA.test(data_test, load_model=model_path, device=DEVICE)\n",
    "    test_pred, test_pol = modelABSA.predict_batch(data_test, load_model=model_path, device=DEVICE)\n",
    "\n",
    "    train_accuracy, train_report = modelABSA.test(data, load_model=model_path, device=DEVICE)\n",
    "    train_pred, train_pol = modelABSA.predict_batch(data, load_model=model_path, device=DEVICE)\n",
    "\n",
    "    #save results\n",
    "    if not os.path.exists(dir_name_s+'/results'):\n",
    "        os.makedirs(dir_name_s+'/results')\n",
    "\n",
    "    #report\n",
    "    with open(dir_name_s+'/results/test_report_lr{}_epochs{}_batch{}.csv'.format(lr, epochs, batch), 'w') as f:\n",
    "        for r in test_report.split('\\n'):\n",
    "            f.write(r + '\\n')\n",
    "\n",
    "    with open(dir_name_s+'/results/train_report_lr{}_epochs{}_batch{}.csv'.format(lr, epochs, batch), 'w') as f:\n",
    "        for r in train_report.split('\\n'):\n",
    "            f.write(r + '\\n')\n",
    "\n",
    "    #predictions\n",
    "    data_test['Predicted'] = test_pred\n",
    "    data_test['Actual'] = test_pol\n",
    "    data_test.to_csv(dir_name_s+'/results/test_pred_lr{}_epochs{}_batch{}.csv'.format(lr, epochs, batch), index=False)\n",
    "\n",
    "    data['Predicted'] = train_pred\n",
    "    data['Actual'] = train_pol\n",
    "    data.to_csv(dir_name_s+'/results/train_pred_lr{}_epochs{}_batch{}.csv'.format(lr, epochs, batch), index=False)\n",
    "\n",
    "    #accuracy\n",
    "    test_accuracy = np.array(test_accuracy)\n",
    "    train_accuracy = np.array(train_accuracy)\n",
    "\n",
    "    with open(dir_name_s+'/results/test_accuracy_lr{}_epochs{}_batch{}.csv'.format(lr, epochs, batch), 'w') as f:\n",
    "        f.write(str(test_accuracy))\n",
    "    with open(dir_name_s+'/results/train_accuracy_lr{}_epochs{}_batch{}.csv'.format(lr, epochs, batch), 'w') as f:\n",
    "        f.write(str(train_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ABSA_test_train(False, False)\n",
    "run_ABSA_test_train(False, True)\n",
    "run_ABSA_test_train(True, False)\n",
    "run_ABSA_test_train(True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a Training history\n",
    "\n",
    "In the following figure, we plot the training loss and the validation loss for the fine-tuning and adapter cases, both with and without scheduler. \n",
    "\n",
    "We can observe that all of the models exhibit a similar behavior, with the same oscillating trend. Note also that the order of magnitude is higher if compared to the ATE model. Finally, let us highlight that in order to limit the overfitting, one should not train the model for more than 5 epochs. Indeed, we tested the training up to 10 epochs and computed the correspondent test accuracy: convergence is achieved after 5 epochs, even though the loss function keeps decreasing, thus it is better to limit the number of epochs chosen when training the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossABTE = np.loadtxt('model_ABSA/losses_lr1e-05_epochs5_batch8.txt')\n",
    "lossABTE_AS = np.loadtxt('model_ABSA_adapter_scheduler/losses_lr1e-05_epochs5_batch8.txt')\n",
    "lossABTE_S = np.loadtxt('model_ABSA_scheduler/losses_lr1e-05_epochs5_batch8.txt')\n",
    "lossABTE_A = np.loadtxt('model_ABSA_adapter/losses_lr1e-05_epochs5_batch8.txt')\n",
    "\n",
    "sns.set_theme (style=\"white\", rc={\"lines.linewidth\": 3}, font_scale=1.5, palette=\"Dark2\")\n",
    "fig, ax = plt.subplots(figsize=(18,5))\n",
    "\n",
    "sns.lineplot(range(len(lossABTE)), lossABTE, ax=ax, label = 'Fine tuning')\n",
    "sns.lineplot(range(len(lossABTE_S)), lossABTE_S, ax=ax, label = 'Fine tuning +\\nscheduler')\n",
    "sns.lineplot(range(len(lossABTE_A)), lossABTE_A, ax=ax, label = 'Adapter')\n",
    "sns.lineplot(range(len(lossABTE_AS)), lossABTE_AS, ax=ax, label = 'Adapter +\\nscheduler')\n",
    "\n",
    "ax.set_xlabel('iteration')\n",
    "ax.set_ylabel('loss')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "if not os.path.isdir('results_ABSA'):\n",
    "    os.makedirs('results_ABSA')\n",
    "\n",
    "fig.savefig('results_ABSA/loss_lr{:.5f}_epochs{}_batch{}.pdf'.format(lr, epochs, batch), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b Classification reports\n",
    "\n",
    "Here we propose the classification reports for all the four models trained in the previous sections and tested on test dataset.\n",
    "\n",
    "Fine-tuning with scheduler and simple adapter show the same results, with high precision and recall for negative and positive aspects, but low recall for neutral aspects, which means that we have difficulties in predicting neutral aspects. Low recall for neutral aspects can be also highlighted in fine-tuning and adapter+scheduler.\n",
    "\n",
    "In general, fine-tuning and adapter+scheduler exhibit lower values of precision/recall (f1-score) if compared with the other two. Therefore, we can conclude that fine-tuning with scheduler and simple adapter are the best models to identify negative and positive polarities, even though they have high proabability of failing in identifying neutral aspects.\n",
    "\n",
    "These results could be easily understood, since the concept of \"neutral aspect\" is less clear than 'positive' and 'negative' polarities, also in a common spoken language. Moreover, if we think of possible applications of the provided architecture, we could argue that a possible customer of the model we build would be more interested in identifying negative and positive aspects, rather than neutral ones. As a consequence, we can conclude that even if it has limitations, the provided model can still be applied with good results to detect whether a sentence is positive or negative with respect to a certain aspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ABSA = classification_report_read('model_ABSA/results/test_report_lr1e-05_epochs5_batch8.csv')\n",
    "test_ABSA_S = classification_report_read('model_ABSA_scheduler/results/test_report_lr1e-05_epochs5_batch8.csv')\n",
    "test_ABSA_A = classification_report_read('model_ABSA_adapter/results/test_report_lr1e-05_epochs5_batch8.csv')\n",
    "test_ABSA_AS = classification_report_read('model_ABSA_adapter_scheduler/results/test_report_lr1e-05_epochs5_batch8.csv')\n",
    "\n",
    "print_aligned(test_ABSA, test_ABSA_S, 'TEST FINE-TUNING', 'TEST FINE-TUNING+SCHEDULER')\n",
    "print_aligned(test_ABSA_A, test_ABSA_AS, 'TEST ADAPTER', 'TRAIN ADAPTER+SCHEDULER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.c Confusion matrix\n",
    "\n",
    "Finally, let us go deeper in the analysis of the provided architecture by showing the confusion matrices for the four models.\n",
    "\n",
    "The main observation is that all of the models show high rate of true positive for positive polarity. Moreover, the most problematic classification concerns the neutral aspect, which is likely misclassified as having positive polarity, and the misclassification of negatives as positives is also non negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix \n",
    "def plot_confusion_matrix(predictions, labels, title, ax, \n",
    "                          cmap='BuPu'):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(labels, predictions, normalize = 'true')\n",
    "    cm = cm[1:,1:]\n",
    "    sns.set_theme (style=\"white\", rc={\"lines.linewidth\": 3}, font_scale=1.5)\n",
    "    sns.heatmap(cm, annot=True,  cmap=cmap, ax=ax, cbar=False)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticklabels(['Negative', 'Neutral', 'Positive'])\n",
    "    ax.set_yticklabels(['Negative', 'Neutral', 'Positive'])\n",
    "    ax.set_ylabel('Ground Truth'),ax.set_xlabel('Predicted')\n",
    "    \n",
    "def plot_confusion_matrix_df(df_path, title, ax, cmap='BuPu'):\n",
    "\n",
    "    df = pd.read_csv(df_path)\n",
    "    pred = df['Predicted'].apply(lambda x: [int (i) if i !='None' else -2 for i in x.strip('][').split(', ')]).to_list()\n",
    "    gtruth = df['Actual'].apply(lambda x: [int (i) if i !='None' else -2 for i in x.strip('][').split(', ')]).to_list()\n",
    "    predicted, ground_truth = [], []\n",
    "    for i in range(len(pred)):\n",
    "        predicted+=pred[i]\n",
    "        ground_truth+=gtruth[i]\n",
    "    plot_confusion_matrix(predicted, ground_truth, title, ax, cmap=cmap)\n",
    "\n",
    "def compare_confusion_mat():\n",
    "    df = 'model_ABSA/results/test_pred_lr1e-05_epochs5_batch8.csv'\n",
    "    dfS = 'model_ABSA_scheduler/results/test_pred_lr1e-05_epochs5_batch8.csv'\n",
    "    dfA = 'model_ABSA_adapter/results/test_pred_lr1e-05_epochs5_batch8.csv'\n",
    "    dfAS = 'model_ABSA_adapter_scheduler/results/test_pred_lr1e-05_epochs5_batch8.csv'\n",
    "    fig, ax = plt.subplots(2,2,figsize=(15,15))\n",
    "    #set space between subplots\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    fig.suptitle('Confusion matrices')\n",
    "    plot_confusion_matrix_df(df, r'$\\bf{FINE-TUNING}$', ax[0][0])\n",
    "    plot_confusion_matrix_df(dfS, r'$\\bf{FINE-TUNING + SCHEDULING}$', ax[0][1])\n",
    "    plot_confusion_matrix_df(dfA, r'$\\bf{ADAPTER}$', ax[1][0])\n",
    "    plot_confusion_matrix_df(dfAS, r'$\\bf{ADAPTER + SCHEDULING}$', ax[1][1])\n",
    "    \n",
    "    fig.savefig('results_ABSA/CMatrix_test.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "compare_confusion_mat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.d Results visualization\n",
    "\n",
    "Finally, let us show how to use the provided model to predict the polarity of a sentence with respect to a specific aspect with any sentence. It is sufficient to call the method predict of the class ABSAModel. Examples as follow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review, aspect, model, model_path):\n",
    "    x, y, z = modelABSA.predict(review, aspect, load_model=model_path)\n",
    "    \n",
    "    if y == 1:\n",
    "        return 'The review \"{}\" \\nw.r.t. the aspect \"{}\" is positive\\n'.format(review, aspect)\n",
    "    elif y == 0:\n",
    "        return 'The review \"{}\" \\nw.r.t. the aspect \"{}\" is neutral\\n'.format(review, aspect)\n",
    "    else:\n",
    "        return 'The review \"{}\" \\nw.r.t. the aspect \"{}\" is negative\\n'.format(review, aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"/home/STU/ljq/Projects/BERT-ABSA/bert-base-uncased\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = 'model_ABSA_adapter_scheduler/model_lr1e-05_epochs4_batch8.pkl'\n",
    "modelABSA = ABSAModel(tokenizer, adapter=True) \n",
    "\n",
    "print (predict_sentiment(\"they make the BEST spice tuna roll in town, and the asian salad is ok\", \"tuna\", modelABSA, model_path)+\"\\n\")\n",
    "\n",
    "print (predict_sentiment(\"the food is fantastic, but the prices were too high\", \"prices\", modelABSA, model_path)+\"\\n\")\n",
    "print (predict_sentiment(\"the food is fantastic, but the prices were too high\", \"food\", modelABSA, model_path)+\"\\n\")\n",
    "\n",
    "print (predict_sentiment(\"the chicken tastes like plastic, even tough they make the best \", \"chicken\", modelABSA, model_path)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word clouds (adapter)\n",
    "\n",
    "Finally, just to have an overview of which are the term words that can be associated with a certain feeling, we plot the word clouds both for negative and positive polarity, showing target vs predicted.\n",
    "Note that, since reviews refer to different restaurants, the same words can be associated with two opposite feelings, e.g. the word 'food' will of course mark both a positive and a negative review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.read_csv('model_ABSA_adapter/results/test_pred_lr1e-05_epochs5_batch8.csv')\n",
    "\n",
    "def tokens_polarity (tokens, polarity_data):\n",
    "\n",
    "    #string to list\n",
    "    tokens = [t.strip('][').split(', ') for t in tokens]\n",
    "    polarity_data = [p.strip('][').split(', ') for p in polarity_data]\n",
    "    polarity_data = [[int(i) if i != 'None' else None for i in p] for p in polarity_data]\n",
    "    positive = []\n",
    "    negative = []\n",
    "    neutral = []\n",
    "    for p in polarity_data: #each p is a list\n",
    "        for pp in p:\n",
    "            t = tokens[polarity_data.index(p)][p.index(pp)][1:-1]\n",
    "            if pp is not None:\n",
    "                if pp == 1:\n",
    "                    positive.append(t)\n",
    "                elif pp == -1:\n",
    "                    negative.append(t)\n",
    "                elif pp == 0:\n",
    "                    neutral.append(t)\n",
    "    return positive, negative, neutral\n",
    "\n",
    "pred_pos, pred_neg, pred_neut = tokens_polarity(df_pred['Tokens'].values, df_pred['Predicted'].values)\n",
    "gold_pos, gold_neg, gold_neut = tokens_polarity(df_pred['Tokens'].values, df_pred['Actual'].values)\n",
    "\n",
    "print ('POSITIVE ASPECTS')\n",
    "target_predicted_wordcloud(' '.join(gold_pos), ' '.join(pred_pos), \"results_ABSA/adapter_positive_wordcloud.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('NEGATIVE ASPECTS')\n",
    "target_predicted_wordcloud(' '.join(gold_neg), ' '.join(pred_neg), \"results_ABSA/adapter_negative_wordcloud.pdf\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusions and comparison with other results.\n",
    "\n",
    "Let us now draw some conclusions with refer to the actual SemEval 2014 task [2].\n",
    "\n",
    "The following table shows results for the aspect-term extraction task:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/ate.png\" width=\"300\" />\n",
    "</p>\n",
    "\n",
    "Recall that our best model (adapter+scheduler) provides an average f1 score of 0.70 on the classification of an aspect-term, which is in the range of the other goups which partecipated to the SemEval.\n",
    "\n",
    "Moreover, the following table shows the f1-score for the aspect-based sentiment analysis task, considering all the possible polarities for all the groups who partecipated to the task:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/f1absa.png\" width=\"180\" />\n",
    "</p>\n",
    "\n",
    "If we consider neutral, positive and negative polarities, our best model (fine-tuning + scheduler and adapter) provides an average f1-score of 0.72, which is in the range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aknowledgements\n",
    "\n",
    "The general structure of the model (i.e. padding, dataset construction...) has been taken from [1], nevetheless we organized the model into a user-friendly class structure which provides a simple interface to the model. Moreover, we changed the optimization strategy, using AdamW instead of a generic Adam, introducing the learning rate scheduling and Adapter option as an alternative to fine-tuning.\n",
    "Finally, we add a detailed analysis of the model performance, including the study of the training history, confusion matrix and visualization\n",
    "\n",
    "\n",
    "## References:\n",
    "\n",
    "[1] **Aspect-Term-Extraction-and-Analysis**, https://github.com/1tangerine1day/Aspect-Term-Extraction-and-Analysis\n",
    "\n",
    "[2] Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. **SemEval-2014 Task 4: Aspect Based Sentiment Analysis.** In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27â€“35, Dublin, Ireland. Association for Computational Linguistics.\n",
    "\n",
    "[3] Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina, **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**, DOI: 10.48550/ARXIV.1810.04805\n",
    "\n",
    "[4] **HuggingFace BERT models**, https://huggingface.co/docs/transformers/model_doc/bert\n",
    "\n",
    "[5] **AdapterHub: A Framework for Adapting Transformers**, Jonas Pfeiffer et al., Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020): Systems Demonstrations, https://www.aclweb.org/anthology/2020.emnlp-demos.7\n",
    "\n",
    "[6] Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and de Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain, **Parameter-Efficient Transfer Learning for NLP**, DOI: 10.48550/ARXIV.1902.00751\n",
    "\n",
    "[7] Loshchilov Ilya, Hutter Frank, **Decoupled Weight Decay Regularization**, DOI: 10.48550/ARXIV.1711.05101\n",
    "\n",
    "[8] PyTorch: An Imperative Style, High-Performance Deep Learning Library}, Paszke Adam,  Gross Sam, Massa Francisco, Lerer Adamm, Bradbury James, Chanan Gregory, Killeen Trevor, Lin Zeming, Gimelshein Natalia, Antiga Luca, Desmaison Alban, Kopf Andreas, Yang Edward, DeVit Zachary, Raison Martin, Tejani Alykhan, Chilamkurthy Sasank, Steiner Benoit, Fang Lu, Bai Junjie, Chintala Soumith, http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert-absa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
